[%
  title='MyMediaLite: How to use the command line programs'
  prefix='../'
%]

[% INCLUDE header %]

<h1><a href="index.html">[% title %]</a></h1>

[% INCLUDE menu %]

[% INCLUDE infobox %]

<div id="content">
<p>
MyMediaLite is mainly a library, meant to be used by other applications.<br/>
We provide two command-line tools that offer much of MyMediaLite's functionality.
They allow you to find out how MyMediaLite can deal with a dataset without having
to integrate the library in your application or having to develop your own program.
</p>
<h2>Rating Prediction</h2>
<p>
  The programs expect the data to be in a simple text format:
  <pre>
  user_id item_id rating
  </pre>
  where user_id and item_id are integers referring to users and items, respectively,
  and rating is a floating-point number expressing how much a user likes an item.
  The whitespace between the values can contain an arbitrary number of space and tab characters.
  If there are more than three columns, all additional columns willsc be ignored.

  A small example dataset:
  <pre>
  1       1       5
  1       2       3
  1       3       4
  1       4       3
  1       5       3
  1       7       4
  </pre>
  </p>
    
  <p>
    The general <a href="rating_prediction.html">usage of the rating prediction program</a> is as follows:
    <pre>
    mono RatingPrediction.exe --training-file=TRAINING_FILE --test-file=TEST_FILE --recommender=METHOD [OPTIONS]
    </pre>
    If you use Windows, just leave out the <tt>mono</tt>.
    METHOD is the recommender to use, which will be trained using the contents
    of TRAINING_FILE. The recommender will then predict the data in TEST_FILE, and the program
    will display the RMSE (root mean square error) and MAE (mean absolute error) of the
    predictions.
  </p>
  
  <p>
    <a href="rating_prediction.html">
      If you call RatingPrediction.exe without arguments, it will provide a list of recommenders
      to choose from, plus their arguments and further options.
    </a>
  </p>
  
  <p>
    You can <a href="http://www.grouplens.org/node/73">download the MovieLens 100k ratings dataset from the
    GroupLens Research website</a> and unzip it to have something to play with.
    If you have downloaded the MyMediaLite source code, you can do this automatically by entering
    <pre>
      make download-movielens
    </pre>
  </p>
  
  <p>
    To try out a simple baseline method on the data, you just need to enter
    <pre>
    mono RatingPrediction.exe --training-file=u1.base --test-file=u1.test --recommender=UserAverage
    </pre>
    which should give a result like
    <pre>
    UserAverage training_time 00:00:00.000098 RMSE 1.063 MAE 0.85019 testing_time 00:00:00.032326
    </pre>
  </p>

  <p>
    To use a more advanced recommender, enter
    <pre>
    mono RatingPrediction.exe --training-file=u1.base --test-file=u1.test --recommenderBiasedMatrixFactorization
    </pre>
    which yields better result than the user average:
    <pre>
    BiasedMatrixFactorization num_factors=10 regularization=0.015 learn_rate=0.01 num_iter=30
                                init_mean=0 init_stdev=0.1
    training_time 00:00:03.3575780 RMSE 0.96108 MAE 0.75124 testing_time 00:00:00.0159740
    </pre>
    The key-value pairs after the method name represent arguments to the recommender
    that may be modified to get even better results. For instance, we could use more latent factors
    per user and item, which leads to a more complex (and hopefully more accurate) model:
    <pre>
    mono RatingPrediction.exe --training-file=u1.base --test-file=u1.test --recommender=BiasedMatrixFactorization --recommender-options="num_factors=20"
    ...
    ... RMSE 0.98029 MAE 0.76558
    </pre>
    Wait a second The RMSE actually got worse!<br/>
    This may be because we do not train our model with the optimal arguments.
    One thing to look at is the number of iterations.
    If we iterate for too long, the learning process overfits the training data,
    which means the resulting model does not generalize well to predict unknown future data.
    The options <tt>find_iter=A</tt>, <tt>num_iter=B</tt> and <tt>max_iter=C</tt> help us
    to find the right number of iterations. Togehter, the three options mean
    &quot;From iteration B on, give out the evaluation results on the test set every A iterations,
    until you reach iteration C.&quot;
    <pre>
    mono RatingPrediction.exe --training-file=u1.base --test-file=u1.test --recommender=BiasedMatrixFactorization --recomender-options="num_factors=20 num_iter=0" --max-iter=25 --num-iter=0
    ...
    RMSE 1.17083 MAE 0.96918 0
    RMSE 1.01383 MAE 0.8143 1
    RMSE 0.98742 MAE 0.78742 2
    RMSE 0.97672 MAE 0.77668 3
    RMSE 0.9709 MAE 0.77078 4
    RMSE 0.96723 MAE 0.76702 5
    RMSE 0.96466 MAE 0.76442 6
    RMSE 0.96269 MAE 0.76241 7
    RMSE 0.96104 MAE 0.76069 8
    RMSE 0.95958 MAE 0.75917 9
    RMSE 0.95825 MAE 0.75783 10
    RMSE 0.95711 MAE 0.75667 11
    RMSE 0.95626 MAE 0.75569 12
    RMSE 0.95578 MAE 0.75501 13
    RMSE 0.95573 MAE 0.75467 14
    RMSE 0.95611 MAE 0.75467 15
    RMSE 0.9569 MAE 0.75499 16
    RMSE 0.95802 MAE 0.75551 17
    RMSE 0.95942 MAE 0.75623 18
    RMSE 0.96102 MAE 0.7571 19
    RMSE 0.96277 MAE 0.75806 20
    RMSE 0.96463 MAE 0.75909 21
    RMSE 0.96656 MAE 0.76017 22
    RMSE 0.96852 MAE 0.7613 23
    RMSE 0.9705 MAE 0.76246 24
    RMSE 0.97247 MAE 0.76364 25    
    </pre>
    This means that we should probably set <tt>--num-iter</tt> to around 15 to get better results.
  </p>
  
  <p>
    <b>Warning:</b>
    Depending on the recommender, the choice of arguments (hyperparameters) may be crucial to
    the recommender's performance.
    Sometimes you may find suitable values by playing a bit with the arguments, starting from their default
    values. However, there is no guarantee that this will work!
    You should use a principled approach to find good hyperparameters, e.g. cross-validation and grid search.
  </p>

  <!-- TODO document load/save -->
  <!-- TODO document cutoff techniques -->

<h2>Item Prediction from Implicit Feedback</h2>
<p>
  The item prediction program behaves similarly to the rating prediction program,
  so we concentrate on the differences here.
</p>

<p>
  <pre>mono ItemPrediction.exe --training-file=TRAINING_FILE --test-file=TEST_FILE --recommender=METHOD [OPTIONS]</pre>
</p>

<p>
  Again,
  <a href="item_prediction.html">
    if you call ItemPrediction.exe without arguments, it will provide a list of recommenders
    to choose from, plus their arguments and further options.
  </a>
  If you use Windows, leave out the <tt>mono</tt> in the call above.
</p>

<p>
  The third column in the data files may be omitted. If it is present, it will be ignored.
</p>

<p>
  Example:
  <pre>
  1       1
  1       2
  1       3
  1       4
  1       5
  1       7
  </pre>
</p>

  <p>
  Instead of RMSE and MAP, the evaluation measures are now prec@N (precision at N), <a href="http://en.wikipedia.org/wiki/Receiver_operating_characteristic">AUC (area under the ROC curve)</a>,
  MAP (mean average precision), and <a href="http://en.wikipedia.org/wiki/Discounted_cumulative_gain">NDCG (normalized discounted cumulative gain)</a>.
  </p>
  
  <p>
  Let us start again with some baseline methods, <tt>Random</tt> and <tt>MostPopular</tt>:
  <pre>
  mono ItemPrediction.exe --training-file=u1.base --test-file=u1.test --recommender=Random
  random training_time 00:00:00.0001040
    AUC 0.49924 prec@5 0.02789 prec@10 0.02898 MAP 0.00122 NDCG 0.37214
    num_users 459 num_items 1650 testing_time 00:00:02.7115540  
  
  mono ItemPrediction.exe --training-file=u1.base --test-file=u1.test --recommender=MostPopular
  MostPopular training_time 00:00:00.0015710
    AUC 0.8543 prec@5 0.322 prec@10 0.30458 MAP 0.02187 NDCG 0.57038
    num_users 459 num_items 1650 testing_time 00:00:02.3813790
  </pre>
  </p>
  
  <p>
  User-based collaborative filtering:
  <pre>
  mono ItemPrediction.exe --training-file=u1.base --test-file=u1.test --recommender=UserKNN
  UserKNN k=80 training_time 00:00:05.6057200
    AUC 0.91682 prec@5 0.52505 prec@10 0.46776 MAP 0.06482 NDCG 0.68793
    num_users 459 num_items 1650 testing_time 00:00:08.8362840
  </pre>
  </p>
  
  <p>  
  Note that the item prediction evaluation usually takes longer than the rating prediction evaluation,
  because for each user, scores for every relevant item (possibly all items) have to be computed.
  You can restrict the number of predictions to be made using the options <tt>predict_for_users=FILE</tt>
  and <tt>relevant_items=FILE</tt> to save time.
  </p>
  
  <p>
  The item prediction program supports the same options (<tt>--find-iter=N</tt> etc.)
  for iteratively trained recommenders like <tt>BPRMF</tt> and <tt>WRMF</tt>.
  </p>
  
</div>

[% INCLUDE footer %]
